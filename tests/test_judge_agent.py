from __future__ import annotations

from src.agents.judge_agent import build_judge_prompt
from src.tools.search import SearchDocsArgs, search_docs


def test_judge_agent_returns_structured_verdict(test_judge_agent):
    prompt = build_judge_prompt(
        question="What is a monomial ideal?",
        answer="A monomial ideal is generated by monomials.",
        references=["[functions\\ideal-doc.m2] definition of ideal"],
        tool_notes=["search_docs -> summarize_docs"],
    )

    result = test_judge_agent.run_sync(prompt)

    assert result.output.decision in {"pass", "fail"}
    assert 0 <= result.output.score <= 1
    assert result.output.rationale


def test_end_to_end_krull_dimension_judging(test_rag_agent, test_judge_agent):
    question = "How can I find the Krull dimension of the ring k[x, y, z]/(x - y^2)?"

    rag_result = test_rag_agent.run_sync(question)
    rag_output = rag_result.output

    assert rag_output.answer, "RAG agent did not produce an answer."

    if not rag_output.references:
        # If the mocked TestModel didn't add references, create a placeholder
        rag_output.references = ["[test_source] placeholder reference"]

    judge_prompt = build_judge_prompt(
        question=question,
        answer=rag_output.answer,
        references=rag_output.references,
        tool_notes=["search_docs", "summarize_docs"],
    )

    verdict = test_judge_agent.run_sync(judge_prompt).output
    assert verdict.decision in {"pass", "fail"}
    assert 0.0 <= verdict.score <= 1.0
    assert verdict.rationale
